{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84353166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import medspacy\n",
    "from medspacy.context import ConText\n",
    "from medspacy.target_matcher import TargetMatcher\n",
    "from cxr_target_rules import rules\n",
    "from typing import Iterable, List, Tuple, Counter\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e923efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'medspacy_pyrush', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'medspacy_target_matcher', 'medspacy_context']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(\"medspacy_pyrush\", before=\"parser\")  # No need to import class\n",
    "nlp.add_pipe(\"medspacy_target_matcher\")\n",
    "nlp.add_pipe(\"medspacy_context\")\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c4a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher = nlp.get_pipe(\"medspacy_target_matcher\")\n",
    "\n",
    "target_rules = rules\n",
    "\n",
    "target_matcher.add(target_rules) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "20018b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(tokens: List[str], n: int) -> Iterable[Tuple[str, ...]]:\n",
    "    if len(tokens) < n:\n",
    "        return []\n",
    "    return zip(*(tokens[i:] for i in range(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llr_2x2(k11, k12, k21, k22) -> float:\n",
    "    def _xlogx(x):\n",
    "        return 0.0 if x == 0 else x * math.log(x)\n",
    "    N = k11 + k12 + k21 + k22\n",
    "    return 2.0 * (\n",
    "        _xlogx(k11) + _xlogx(k12) + _xlogx(k21) + _xlogx(k22)\n",
    "        - _xlogx(k11 + k12) - _xlogx(k21 + k22)\n",
    "        - _xlogx(k11 + k21) - _xlogx(k12 + k22)\n",
    "        + _xlogx(N)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "165d338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_counts(doc, unigram_counts: Counter, ngram_counts, max_n=5):\n",
    "    def normalize(tok):\n",
    "        return tok.text.lower()\n",
    "    \n",
    "    total_tokens = 0\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        keep = False\n",
    "        for ent in doc.ents:\n",
    "            if (ent.start_char >= sent.start_char and ent.end_char <= sent.end_char):\n",
    "                keep = True\n",
    "                break\n",
    "        if not keep:\n",
    "            continue\n",
    "\n",
    "        tokens = [tok for tok in sent if not tok.is_space and not tok.is_punct and not tok.is_stop]\n",
    "        valid_unigrams = [tok for tok in tokens if not tok.is_stop and tok.pos_ in {\"NOUN\", \"ADJ\", \"PROPN\"}]\n",
    "        unigram_counts.update(normalize(tok) for tok in valid_unigrams)\n",
    "        total_tokens = len(valid_unigrams)\n",
    "\n",
    "        for n in range(2, max_n + 1):\n",
    "            for gram in ngrams(valid_unigrams, n):\n",
    "                if all(tok.is_stop for tok in gram): # type: ignore\n",
    "                    continue\n",
    "                if gram[0].lower_ in {\"the\", \"a\", \"an\", \"is\", \"was\", \"are\", \"there\", \"to\"}: #type: ignore\n",
    "                    continue\n",
    "                if not any(tok.pos_ in {\"NOUN\", \"ADJ\", \"PROPN\"} for tok in gram): # type: ignore\n",
    "                    continue\n",
    "\n",
    "                phrase = tuple(normalize(tok) for tok in gram)\n",
    "                ngram_counts[n].update([phrase])\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0b8be532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bigrams(unigram_counts: Counter, bigram_counts: Counter, total_tokens: int, min_freq=3,):\n",
    "    total_unigrams = total_tokens\n",
    "    bigram_llr = {}\n",
    "    for (w1, w2), k11 in bigram_counts.items():\n",
    "        if k11 < min_freq:\n",
    "            continue\n",
    "        k1_ = unigram_counts[w1]\n",
    "        k_1 = unigram_counts[w2]\n",
    "        k12 = k1_ - k11\n",
    "        k21 = k_1 - k11\n",
    "        k22 = total_unigrams - (k11 + k12 + k21)\n",
    "        score = llr_2x2(k11, k12, k21, k22)\n",
    "        bigram_llr[(w1, w2)] = round(score, 3)\n",
    "    return bigram_llr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "25ceb6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ngrams_to_csv(\n",
    "    output_csv: str,\n",
    "    unigram_counts: Counter,\n",
    "    ngram_counts: dict,\n",
    "    bigram_llr: dict,\n",
    "    min_freq=3\n",
    "):\n",
    "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"phrase\", \"n\", \"freq\", \"llr\"])\n",
    "\n",
    "        for w, c in unigram_counts.most_common():\n",
    "            if c < min_freq:\n",
    "                continue\n",
    "            writer.writerow([w, 1, c, \"\"])\n",
    "\n",
    "        for n, counter in ngram_counts.items():\n",
    "            for gram, c in counter.most_common():\n",
    "                if c < min_freq:\n",
    "                    continue\n",
    "                phrase = \" \".join(gram)\n",
    "                llr_val = bigram_llr.get(gram, \"\") if n == 2 else \"\"\n",
    "                writer.writerow([phrase, n, c, llr_val])\n",
    "\n",
    "    print(f\"Saved n-gram data to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c71630f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save(jsonl_path, output_csv, max_n=3, min_freq=5):\n",
    "    with open(jsonl_path, \"r\") as infile:\n",
    "        texts = [json.loads(line)[\"text\"] for line in infile if \"text\" in line]\n",
    "\n",
    "    unigram_counts = Counter()\n",
    "    ngram_counts = {n: Counter() for n in range(2, max_n + 1)}\n",
    "    total_tokens = 0\n",
    "\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=32), desc=\"ğŸ” Processing docs\"):\n",
    "        total_tokens += extract_counts(doc, unigram_counts, ngram_counts, max_n=max_n)\n",
    "\n",
    "    # print(\"Unigrams:\", unigram_counts.most_common())\n",
    "    # print(\"Bigrams:\", ngram_counts[2].most_common())\n",
    "    bigram_llr = score_bigrams(unigram_counts, ngram_counts[2], total_tokens, min_freq=min_freq)\n",
    "    write_ngrams_to_csv(output_csv, unigram_counts, ngram_counts, bigram_llr, min_freq=min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8fb61110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ” Processing docs: 1000it [00:19, 52.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved n-gram data to ./mimic_cxr_training_labelled_vocab.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_reports = \"./mimic_cxr_reports.jsonl\" \n",
    "train_reports = \"./mimic_cxr_train_val.jsonl\"\n",
    "output_csv = \"./mimic_cxr_vocab.csv\"\n",
    "training_vocab = \"./mimic_cxr_training_labelled_vocab.csv\"\n",
    "process_and_save(train_reports, training_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
