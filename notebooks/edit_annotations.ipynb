{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97403e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import medspacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "from spacy.util import filter_spans\n",
    "from notebooks.cxr_target_rules import rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60dcdda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = medspacy.load()\n",
    "\n",
    "context = nlp.get_pipe(\"medspacy_context\")\n",
    "\n",
    "target_matcher = nlp.get_pipe(\"medspacy_target_matcher\")\n",
    "\n",
    "target_rules = rules\n",
    "\n",
    "target_matcher.add(target_rules) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcb08fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 documents.\n"
     ]
    }
   ],
   "source": [
    "with open(\"doccano_input.jsonl\") as f:\n",
    "    examples = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Loaded {len(examples)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d39807e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(example):\n",
    "    doc = nlp(example[\"text\"])\n",
    "\n",
    "    if not spacy.tokens.Span.has_extension(\"context_label\"):\n",
    "        spacy.tokens.Span.set_extension(\"context_label\", default=\"PRESENT\")\n",
    "\n",
    "    # Convert annotations to Span objects\n",
    "    ents = []\n",
    "    for ann in example[\"annotations\"]:\n",
    "        span = doc.char_span(ann[\"start_offset\"], ann[\"end_offset\"], label=ann[\"label\"])\n",
    "        if span:\n",
    "            ents.append(span)\n",
    "    ents = filter_spans(ents)\n",
    "    doc.ents = ents\n",
    "    context = nlp.get_pipe(\"medspacy_context\")\n",
    "    context(doc)\n",
    "    \n",
    "    new_ents = []\n",
    "    for span in doc.ents:\n",
    "        if span._.is_negated:\n",
    "            context_label = \"NEGATED\"\n",
    "        elif span._.is_uncertain:\n",
    "            context_label = \"UNCERTAIN\"\n",
    "        else:\n",
    "            context_label = \"PRESENT\"\n",
    "        # Update label to include context\n",
    "        span.label_ = f\"{span.label_} ({context_label})\"\n",
    "        new_ents.append(span)\n",
    "\n",
    "    doc.ents = new_ents\n",
    "\n",
    "    colors = {\n",
    "        \"FINDING (PRESENT)\": \"lightgreen\",\n",
    "        \"FINDING (NEGATED)\": \"lightcoral\",\n",
    "        \"FINDING (UNCERTAIN)\": \"gold\",\n",
    "        \"ANATOMY (PRESENT)\": \"lightgreen\",\n",
    "        \"ANATOMY (NEGATED)\": \"lightcoral\",\n",
    "        \"ANATOMY (UNCERTAIN)\": \"gold\",\n",
    "        \"DEVICE (PRESENT)\": \"lightgreen\",\n",
    "        \"DEVICE (NEGATED)\": \"lightcoral\",\n",
    "        \"DEVICE (UNCERTAIN)\": \"gold\",\n",
    "    }\n",
    "\n",
    "    options = {\"ents\": [ent.label_ for ent in doc.ents], \"colors\": colors}\n",
    "\n",
    "    # Visualize in Jupyter\n",
    "    return displacy.render(doc, style=\"ent\", options=options, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c7f7e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-17 11:58:03.514\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 0 'FINAL' marked as sentence start (span begin)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.515\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 11 '\n",
      " \n",
      " ' marked as sentence start (span end whitespace)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.515\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] GAP DETECTED: tokens 11-11 (idx 47-47) between spans 47-50\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.515\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 11 '\n",
      " \n",
      " ' marked as sentence start (whitespace in gap between spans)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.516\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 34 '+' marked as sentence start (span end next token)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.516\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] GAP DETECTED: tokens 34-34 (idx 120-120) between spans 119-121\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.516\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 34 '+' marked as sentence start (first token in gap between spans)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 35 'Crackles' marked as sentence start (span begin)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 40 'No' marked as sentence start (span end next token)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 40 'No' marked as sentence start (span begin)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.518\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 48 ' ' marked as sentence start (span end whitespace)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.520\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] GAP DETECTED: tokens 48-48 (idx 175-175) between spans 174-176\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.521\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 48 ' ' marked as sentence start (whitespace in gap between spans)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.522\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 49 '/' marked as sentence start (span begin)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.522\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 63 '\n",
      " \n",
      " ' marked as sentence start (span end whitespace)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.522\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] GAP DETECTED: tokens 63-63 (idx 255-255) between spans 255-258\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.523\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 63 '\n",
      " \n",
      " ' marked as sentence start (whitespace in gap between spans)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.523\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 66 '\n",
      " \n",
      " ' marked as sentence start (span end whitespace)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.524\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] GAP DETECTED: tokens 66-66 (idx 271-271) between spans 270-274\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 66 '\n",
      " \n",
      " ' marked as sentence start (whitespace in gap between spans)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 100 'The' marked as sentence start (span end next token)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.526\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 100 'The' marked as sentence start (span begin)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.526\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 120 'In' marked as sentence start (span end next token)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.526\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 120 'In' marked as sentence start (span begin)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.527\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 145 'This' marked as sentence start (span end next token)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.527\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 145 'This' marked as sentence start (span begin)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.527\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 154 '\n",
      " ' marked as sentence start (span end whitespace)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.530\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] GAP DETECTED: tokens 154-154 (idx 739-739) between spans 739-741\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.530\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 154 '\n",
      " ' marked as sentence start (whitespace in gap between spans)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.532\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] [doc 0] Token 155 'Extensive' marked as sentence start (span begin)\u001b[0m\n",
      "\u001b[32m2025-09-17 11:58:03.534\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mPyRuSH.PyRuSHSentencizer\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1m[cpredict_split_gaps|call_id=4] Token/tag mapping: [(FINAL, True), (REPORT, False), (\n",
      " , False), (EXAMINATION, False), (:, False), ( , False), (CHEST, False), ((, False), (PORTABLE, False), (AP, False), (), False), (\n",
      " \n",
      " , True), (INDICATION, False), (:, False), ( , False), (_, False), (_, False), (_, False), (year, False), (old, False), (woman, False), (POD, False), (#, False), (_, False), (_, False), (_, False), (S, False), (/, False), (P, False), (Left, False), (femoral, False), (neck, False), (ORIF, False), (., False), (+, True), (Crackles, True), (on, False), (\n",
      " , False), (exam, False), (., False), (No, True), (SOB, False), (., False), (low, False), (grade, False), (fever, False), (overnight, False), (., False), ( , True), (/, True), (/, False), (evaluate, False), (for, False), (atelectasis, False), (,, False), (\n",
      " , False), (pneumonia, False), (    , False), (evaluate, False), (for, False), (atelectasis, False), (,, False), (pneumonia, False), (\n",
      " \n",
      " , True), (IMPRESSION, False), (:, False), (\n",
      " \n",
      " , True), (In, False), (comparison, False), (with, False), (the, False), (study, False), (of, False), (_, False), (_, False), (_, False), (from, False), (an, False), (outside, False), (facility, False), (,, False), (there, False), (is, False), (\n",
      " , False), (again, False), (enlargement, False), (of, False), (the, False), (cardiac, False), (silhouette, False), (with, False), (suggestion, False), (of, False), (some, False), (central, False), (\n",
      " , False), (pulmonary, False), (vascular, False), (congestion, False), (., False), (The, True), (hemidiaphragms, False), (are, False), (not, False), (well, False), (seen, False), (,, False), (raising, False), (\n",
      " , False), (the, False), (possibility, False), (of, False), (a, False), (small, False), (pleural, False), (effusion, False), (and, False), (compressive, False), (atelectasis, False), (., False), (In, True), (\n",
      " , False), (the, False), (left, False), (mid, False), (to, False), (lower, False), (zone, False), (,, False), (there, False), (is, False), (a, False), (suggestion, False), (of, False), (a, False), (somewhat, False), (ill, False), (-, False), (defined, False), (\n",
      " , False), (area, False), (of, False), (increased, False), (opacification, False), (., False), (This, True), (could, False), (possibly, False), (represent, False), (a, False), (pulmonary, False), (\n",
      " , False), (nodule, False), (., False), (\n",
      " , True), (Extensive, True), (posttraumatic, False), (changes, False), (are, False), (seen, False), (in, False), (the, False), (right, False), (ribs, False), (as, False), (well, False), (as, False), (\n",
      " , False), (dislocation, False), (about, False), (the, False), (right, False), (shoulder, False), (joint, False), (., False)]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">FINAL REPORT<br> EXAMINATION:  CHEST (PORTABLE AP)<br> <br> INDICATION:  ___ year old woman POD #___ S/P Left femoral neck ORIF. +Crackles on<br> exam. No SOB. low grade fever overnight.  // evaluate for \n",
       "<mark class=\"entity\" style=\"background: lightcoral; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    atelectasis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FINDING (NEGATED)</span>\n",
       "</mark>\n",
       ",<br> \n",
       "<mark class=\"entity\" style=\"background: lightcoral; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    pneumonia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FINDING (NEGATED)</span>\n",
       "</mark>\n",
       "     evaluate for \n",
       "<mark class=\"entity\" style=\"background: lightcoral; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    atelectasis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FINDING (NEGATED)</span>\n",
       "</mark>\n",
       ", pneumonia<br> <br> IMPRESSION: <br> <br> In comparison with the study of ___ from an outside facility, there is<br> again enlargement of the \n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cardiac silhouette\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ANATOMY (PRESENT)</span>\n",
       "</mark>\n",
       " with suggestion of some central<br> \n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    pulmonary vascular congestion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FINDING (PRESENT)</span>\n",
       "</mark>\n",
       ". The \n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    hemidiaphragms\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ANATOMY (PRESENT)</span>\n",
       "</mark>\n",
       " are not well seen, raising<br> the possibility of a small \n",
       "<mark class=\"entity\" style=\"background: lightcoral; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    pleural effusion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FINDING (NEGATED)</span>\n",
       "</mark>\n",
       " and compressive \n",
       "<mark class=\"entity\" style=\"background: lightcoral; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    atelectasis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FINDING (NEGATED)</span>\n",
       "</mark>\n",
       ". In<br> the left mid to lower zone, there is a suggestion of a somewhat ill-defined<br> area of increased \n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    opacification\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FINDING (PRESENT)</span>\n",
       "</mark>\n",
       ". This could possibly represent a pulmonary<br> \n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nodule\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FINDING (PRESENT)</span>\n",
       "</mark>\n",
       ".<br> Extensive posttraumatic changes are seen in the right \n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ribs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ANATOMY (PRESENT)</span>\n",
       "</mark>\n",
       " as well as<br> dislocation about the right shoulder joint.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(examples[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: add or fix one annotation manually\n",
    "examples[2][\"annotations\"].append({\n",
    "    \"start_offset\": 10,\n",
    "    \"end_offset\": 15,\n",
    "    \"label\": \"ANATOMY\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5965e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples[0][\"annotations\"].pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd9836",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corrected_doccano_input.jsonl\", \"w\") as f:\n",
    "    for ex in examples:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "print(\"âœ… Saved corrected file to corrected_doccano_input.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxr_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
